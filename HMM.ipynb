{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cdb3f85",
   "metadata": {},
   "source": [
    "# Magics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3dba4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f6d063e",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f1b259",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import time\n",
    "from os.path import exists\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pyro\n",
    "import pyro.contrib.examples.polyphonic_data_loader as poly\n",
    "import pyro.distributions as dist\n",
    "import pyro.poutine as poutine\n",
    "from pyro.distributions import TransformedDistribution\n",
    "from pyro.distributions.transforms import affine_autoregressive\n",
    "from pyro.infer import (\n",
    "    SVI,\n",
    "    JitTrace_ELBO,\n",
    "    Trace_ELBO,\n",
    "    TraceEnum_ELBO,\n",
    "    TraceTMC_ELBO,\n",
    "    config_enumerate,\n",
    ")\n",
    "from pyro.optim import ClippedAdam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ef193a",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1f7ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_path = Path(\"\")\n",
    "emission_matrix_path = Path(\"\")\n",
    "dataset_path = Path(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c55a88c",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0a56c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Emitter(nn.Module):\n",
    "    \"\"\"\n",
    "    Parameterizes the bernoulli observation likelihood `p(x_t | z_t)`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, emission_matrix):\n",
    "        super().__init__()\n",
    "        self.emission_matrix = emission_matrix\n",
    "\n",
    "    def forward(self, z_t):\n",
    "        \"\"\"\n",
    "        Given the latent z at a particular time step t we return the vector of\n",
    "        probabilities `ps` that parameterizes the bernoulli distribution `p(x_t|z_t)`\n",
    "        \"\"\"\n",
    "        ps = torch.mm(self.emission_matrix, z_t)\n",
    "        return ps\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37f6439",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Transition(nn.Module):\n",
    "    \"\"\"\n",
    "    Parameterizes the bernoulli latent transition probability `p(z_t | z_{t-1})`\n",
    "    See section 5 in the reference for comparison.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, transition_matrix):\n",
    "        super().__init__()\n",
    "        self.transition_matrix = torch.nn.Parameter(t.tensor(transition_matrix), requires_grad=True).cuda()\n",
    "\n",
    "    def forward(self, z_t_1):\n",
    "        \"\"\"\n",
    "        Given the latent `z_{t-1}` corresponding to the time step t-1\n",
    "        we return the mean and scale vectors that parameterize the\n",
    "        bernoulli distribution `p(z_t | z_{t-1})`\n",
    "        \"\"\"\n",
    "        \n",
    "        ps = torch.mm(self.transition_matrix, z_t_1)\n",
    "        \n",
    "        return ps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ae8938",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Combiner(nn.Module):\n",
    "    \"\"\"\n",
    "    Parameterizes `q(z_t | z_{t-1}, x_{t:T})`, which is the basic building block\n",
    "    of the guide (i.e. the variational distribution). The dependence on `x_{t:T}` is\n",
    "    through the hidden state of the RNN (see the PyTorch module `rnn` below)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, z_dim, rnn_dim):\n",
    "        super().__init__()\n",
    "        # initialize the three linear transformations used in the neural network\n",
    "        self.lin_z_to_hidden = nn.Linear(z_dim, rnn_dim)\n",
    "        self.lin_hidden_to_loc = nn.Linear(rnn_dim, z_dim)\n",
    "        self.lin_hidden_to_scale = nn.Linear(rnn_dim, z_dim)\n",
    "        # initialize the two non-linearities used in the neural network\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, z_t_1, h_rnn):\n",
    "        \"\"\"\n",
    "        Given the latent z at at a particular time step t-1 as well as the hidden\n",
    "        state of the RNN `h(x_{t:T})` we return the mean and scale vectors that\n",
    "        parameterize the (diagonal) gaussian distribution `q(z_t | z_{t-1}, x_{t:T})`\n",
    "        \"\"\"\n",
    "        # combine the rnn hidden state with a transformed version of z_t_1\n",
    "        h_combined = 0.5 * (self.tanh(self.lin_z_to_hidden(z_t_1)) + h_rnn)\n",
    "        # use the combined hidden state to compute the mean used to sample z_t\n",
    "        loc = self.lin_hidden_to_loc(h_combined)\n",
    "        # use the combined hidden state to compute the scale used to sample z_t\n",
    "        scale = self.softplus(self.lin_hidden_to_scale(h_combined))\n",
    "        # return loc, scale which can be fed into Normal\n",
    "        return loc, scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff740235",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class DMM(nn.Module):\n",
    "    \"\"\"\n",
    "    This PyTorch Module encapsulates the model as well as the\n",
    "    variational distribution (the guide) for the Deep Markov Model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim=88, z_dim=100, emission_dim=100,\n",
    "                 transition_dim=200, rnn_dim=600, num_layers=1, rnn_dropout_rate=0.0,\n",
    "                 num_iafs=0, iaf_dim=50, use_cuda=False):\n",
    "        super().__init__()\n",
    "        # instantiate PyTorch modules used in the model and guide below\n",
    "        self.emitter = Emitter(input_dim, z_dim, emission_dim)\n",
    "        self.trans = GatedTransition(z_dim, transition_dim)\n",
    "        self.combiner = Combiner(z_dim, rnn_dim)\n",
    "        # dropout just takes effect on inner layers of rnn\n",
    "        rnn_dropout_rate = 0. if num_layers == 1 else rnn_dropout_rate\n",
    "        self.rnn = nn.RNN(input_size=input_dim, hidden_size=rnn_dim, nonlinearity='relu',\n",
    "                          batch_first=True, bidirectional=False, num_layers=num_layers,\n",
    "                          dropout=rnn_dropout_rate)\n",
    "\n",
    "        # if we're using normalizing flows, instantiate those too\n",
    "        self.iafs = [affine_autoregressive(z_dim, hidden_dims=[iaf_dim]) for _ in range(num_iafs)]\n",
    "        self.iafs_modules = nn.ModuleList(self.iafs)\n",
    "\n",
    "        # define a (trainable) parameters z_0 and z_q_0 that help define the probability\n",
    "        # distributions p(z_1) and q(z_1)\n",
    "        # (since for t = 1 there are no previous latents to condition on)\n",
    "        self.z_0 = nn.Parameter(torch.zeros(z_dim))\n",
    "        self.z_q_0 = nn.Parameter(torch.zeros(z_dim))\n",
    "        # define a (trainable) parameter for the initial hidden state of the rnn\n",
    "        self.h_0 = nn.Parameter(torch.zeros(1, 1, rnn_dim))\n",
    "\n",
    "        self.use_cuda = use_cuda\n",
    "        # if on gpu cuda-ize all PyTorch (sub)modules\n",
    "        if use_cuda:\n",
    "            self.cuda()\n",
    "\n",
    "    # the model p(x_{1:T} | z_{1:T}) p(z_{1:T})\n",
    "    def model(self, mini_batch, mini_batch_reversed, mini_batch_mask,\n",
    "              mini_batch_seq_lengths, annealing_factor=1.0):\n",
    "\n",
    "        # this is the number of time steps we need to process in the mini-batch\n",
    "        T_max = mini_batch.size(1)\n",
    "\n",
    "        # register all PyTorch (sub)modules with pyro\n",
    "        # this needs to happen in both the model and guide\n",
    "        pyro.module(\"hmm\", self)\n",
    "\n",
    "        # set z_prev = z_0 to setup the recursive conditioning in p(z_t | z_{t-1})\n",
    "        z_prev = self.z_0.expand(mini_batch.size(0), self.z_0.size(0))\n",
    "\n",
    "        # we enclose all the sample statements in the model in a plate.\n",
    "        # this marks that each datapoint is conditionally independent of the others\n",
    "        with pyro.plate(\"z_minibatch\", len(mini_batch)):\n",
    "            # sample the latents z and observed x's one time step at a time\n",
    "            # we wrap this loop in pyro.markov so that TraceEnum_ELBO can use multiple samples from the guide at each z\n",
    "            for t in pyro.markov(range(1, T_max + 1)):\n",
    "                # the next chunk of code samples z_t ~ p(z_t | z_{t-1})\n",
    "                # note that (both here and elsewhere) we use poutine.scale to take care\n",
    "                # of KL annealing. we use the mask() method to deal with raggedness\n",
    "                # in the observed data (i.e. different sequences in the mini-batch\n",
    "                # have different lengths)\n",
    "\n",
    "                # first compute the parameters of the diagonal bernoulli distribution p(z_t | z_{t-1})\n",
    "                transition_probs_t_1 = self.trans(z_prev)\n",
    "\n",
    "                # then sample z_t according to dist.Normal(z_loc, z_scale)\n",
    "                # note that we use the reshape method so that the univariate Normal distribution\n",
    "                # is treated as a multivariate Normal distribution with a diagonal covariance.\n",
    "                with poutine.scale(scale=annealing_factor):\n",
    "                    z_t = pyro.sample(\"z_%d\" % t,\n",
    "                                      dist.Bernoulli(transition_probs_t_1)\n",
    "                                          .mask(mini_batch_mask[:, t - 1:t])\n",
    "                                          .to_event(1))\n",
    "\n",
    "                # compute the probabilities that parameterize the bernoulli likelihood\n",
    "                emission_probs_t = self.emitter(z_t)\n",
    "                # the next statement instructs pyro to observe x_t according to the\n",
    "                # bernoulli distribution p(x_t|z_t)\n",
    "                pyro.sample(\"obs_x_%d\" % t,\n",
    "                            dist.Bernoulli(emission_probs_t)\n",
    "                                .mask(mini_batch_mask[:, t - 1:t])\n",
    "                                .to_event(1),\n",
    "                            obs=mini_batch[:, t - 1, :])\n",
    "                # the latent sampled at this time step will be conditioned upon\n",
    "                # in the next time step so keep track of it\n",
    "                z_prev = z_t\n",
    "\n",
    "    # the guide q(z_{1:T} | x_{1:T}) (i.e. the variational distribution)\n",
    "    def guide(self, mini_batch, mini_batch_reversed, mini_batch_mask,\n",
    "              mini_batch_seq_lengths, annealing_factor=1.0):\n",
    "\n",
    "        # this is the number of time steps we need to process in the mini-batch\n",
    "        T_max = mini_batch.size(1)\n",
    "        # register all PyTorch (sub)modules with pyro\n",
    "        pyro.module(\"hmm\", self)\n",
    "\n",
    "        # if on gpu we need the fully broadcast view of the rnn initial state\n",
    "        # to be in contiguous gpu memory\n",
    "        h_0_contig = self.h_0.expand(1, mini_batch.size(0), self.rnn.hidden_size).contiguous()\n",
    "        # push the observed x's through the rnn;\n",
    "        # rnn_output contains the hidden state at each time step\n",
    "        rnn_output, _ = self.rnn(mini_batch_reversed, h_0_contig)\n",
    "        # reverse the time-ordering in the hidden state and un-pack it\n",
    "        rnn_output = poly.pad_and_reverse(rnn_output, mini_batch_seq_lengths)\n",
    "        # set z_prev = z_q_0 to setup the recursive conditioning in q(z_t |...)\n",
    "        z_prev = self.z_q_0.expand(mini_batch.size(0), self.z_q_0.size(0))\n",
    "\n",
    "        # we enclose all the sample statements in the guide in a plate.\n",
    "        # this marks that each datapoint is conditionally independent of the others.\n",
    "        with pyro.plate(\"z_minibatch\", len(mini_batch)):\n",
    "            # sample the latents z one time step at a time\n",
    "            # we wrap this loop in pyro.markov so that TraceEnum_ELBO can use multiple samples from the guide at each z\n",
    "            for t in pyro.markov(range(1, T_max + 1)):\n",
    "                # the next two lines assemble the distribution q(z_t | z_{t-1}, x_{t:T})\n",
    "                z_loc, z_scale = self.combiner(z_prev, rnn_output[:, t - 1, :])\n",
    "\n",
    "                # if we are using normalizing flows, we apply the sequence of transformations\n",
    "                # parameterized by self.iafs to the base distribution defined in the previous line\n",
    "                # to yield a transformed distribution that we use for q(z_t|...)\n",
    "                if len(self.iafs) > 0:\n",
    "                    z_dist = TransformedDistribution(dist.Normal(z_loc, z_scale), self.iafs)\n",
    "                    assert z_dist.event_shape == (self.z_q_0.size(0),)\n",
    "                    assert z_dist.batch_shape[-1:] == (len(mini_batch),)\n",
    "                else:\n",
    "                    z_dist = dist.Normal(z_loc, z_scale)\n",
    "                    assert z_dist.event_shape == ()\n",
    "                    assert z_dist.batch_shape[-2:] == (len(mini_batch), self.z_q_0.size(0))\n",
    "\n",
    "                # sample z_t from the distribution z_dist\n",
    "                with pyro.poutine.scale(scale=annealing_factor):\n",
    "                    if len(self.iafs) > 0:\n",
    "                        # in output of normalizing flow, all dimensions are correlated (event shape is not empty)\n",
    "                        z_t = pyro.sample(\"z_%d\" % t,\n",
    "                                          z_dist.mask(mini_batch_mask[:, t - 1]))\n",
    "                    else:\n",
    "                        # when no normalizing flow used, \".to_event(1)\" indicates latent dimensions are independent\n",
    "                        z_t = pyro.sample(\"z_%d\" % t,\n",
    "                                          z_dist.mask(mini_batch_mask[:, t - 1:t])\n",
    "                                          .to_event(1))\n",
    "                # the latent sampled at this time step will be conditioned upon in the next time step\n",
    "                # so keep track of it\n",
    "                z_prev = z_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25834da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset loading\n",
    "def load_sequences(dataset_path, model_path):\n",
    "    sequences = {\"train\": {}}\n",
    "    \n",
    "    paths = [path for path in dataset_path.glob(\"*.png\")]\n",
    "    \n",
    "    sequence_groups = {}\n",
    "    for path in paths:\n",
    "        particle = ...\n",
    "        frame = ...\n",
    "        \n",
    "        sequence_groups[particle][frame] = imload(path)\n",
    "    \n",
    "    \n",
    "    sequence_lengths = []\n",
    "    \n",
    "    sequences = []\n",
    "    \n",
    "    \n",
    "    sequences[\"train\"][\"sequence_lengths\"] = \n",
    "    sequences[\"train\"][\"sequences\"] = \n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1c1888",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# setup, training, and evaluation\n",
    "def main(args):\n",
    "    # setup logging\n",
    "    logging.basicConfig(level=logging.DEBUG, format='%(message)s', filename=args.log, filemode='w')\n",
    "    console = logging.StreamHandler()\n",
    "    console.setLevel(logging.INFO)\n",
    "    logging.getLogger('').addHandler(console)\n",
    "    logging.info(args)\n",
    "    \n",
    "    \n",
    "    # Load and annotate the data\n",
    "    data = load_sequences(args.dataset_path)\n",
    "    training_seq_lengths = data['train']['sequence_lengths']\n",
    "    training_data_sequences = data['train']['sequences']\n",
    "    N_train_data = len(training_seq_lengths)\n",
    "    N_train_time_slices = float(torch.sum(training_seq_lengths))\n",
    "    N_mini_batches = int(N_train_data / args.mini_batch_size +\n",
    "                         int(N_train_data % args.mini_batch_size > 0))\n",
    "\n",
    "    logging.info(\"N_train_data: %d     avg. training seq. length: %.2f    N_mini_batches: %d\" %\n",
    "                 (N_train_data, training_seq_lengths.float().mean(), N_mini_batches))\n",
    "\n",
    "    # how often we do validation/test evaluation during training\n",
    "    val_test_frequency = 50\n",
    "    # the number of samples we use to do the evaluation\n",
    "    n_eval_samples = 1\n",
    "\n",
    "    # package repeated copies of val/test data for faster evaluation\n",
    "    # (i.e. set us up for vectorization)\n",
    "    def rep(x):\n",
    "        rep_shape = torch.Size([x.size(0) * n_eval_samples]) + x.size()[1:]\n",
    "        repeat_dims = [1] * len(x.size())\n",
    "        repeat_dims[0] = n_eval_samples\n",
    "        return x.repeat(repeat_dims).reshape(n_eval_samples, -1).transpose(1, 0).reshape(rep_shape)\n",
    "\n",
    "    # instantiate the dmm\n",
    "    dmm = DMM(rnn_dropout_rate=args.rnn_dropout_rate, num_iafs=args.num_iafs,\n",
    "              iaf_dim=args.iaf_dim, use_cuda=args.cuda)\n",
    "\n",
    "    # setup optimizer\n",
    "    adam_params = {\"lr\": args.learning_rate, \"betas\": (args.beta1, args.beta2),\n",
    "                   \"clip_norm\": args.clip_norm, \"lrd\": args.lr_decay,\n",
    "                   \"weight_decay\": args.weight_decay}\n",
    "    adam = ClippedAdam(adam_params)\n",
    "\n",
    "    # setup inference algorithm\n",
    "    if args.tmc:\n",
    "        if args.jit:\n",
    "            raise NotImplementedError(\"no JIT support yet for TMC\")\n",
    "        tmc_loss = TraceTMC_ELBO()\n",
    "        dmm_guide = config_enumerate(dmm.guide, default=\"parallel\", num_samples=args.tmc_num_samples, expand=False)\n",
    "        svi = SVI(dmm.model, dmm_guide, adam, loss=tmc_loss)\n",
    "    elif args.tmcelbo:\n",
    "        if args.jit:\n",
    "            raise NotImplementedError(\"no JIT support yet for TMC ELBO\")\n",
    "        elbo = TraceEnum_ELBO()\n",
    "        dmm_guide = config_enumerate(dmm.guide, default=\"parallel\", num_samples=args.tmc_num_samples, expand=False)\n",
    "        svi = SVI(dmm.model, dmm_guide, adam, loss=elbo)\n",
    "    else:\n",
    "        elbo = JitTrace_ELBO() if args.jit else Trace_ELBO()\n",
    "        svi = SVI(dmm.model, dmm.guide, adam, loss=elbo)\n",
    "\n",
    "    # now we're going to define some functions we need to form the main training loop\n",
    "\n",
    "    # saves the model and optimizer states to disk\n",
    "    def save_checkpoint():\n",
    "        logging.info(\"saving model to %s...\" % args.save_model)\n",
    "        torch.save(dmm.state_dict(), args.save_model)\n",
    "        logging.info(\"saving optimizer states to %s...\" % args.save_opt)\n",
    "        adam.save(args.save_opt)\n",
    "        logging.info(\"done saving model and optimizer checkpoints to disk.\")\n",
    "\n",
    "    # loads the model and optimizer states from disk\n",
    "    def load_checkpoint():\n",
    "        assert exists(args.load_opt) and exists(args.load_model), \\\n",
    "            \"--load-model and/or --load-opt misspecified\"\n",
    "        logging.info(\"loading model from %s...\" % args.load_model)\n",
    "        dmm.load_state_dict(torch.load(args.load_model))\n",
    "        logging.info(\"loading optimizer states from %s...\" % args.load_opt)\n",
    "        adam.load(args.load_opt)\n",
    "        logging.info(\"done loading model and optimizer states.\")\n",
    "\n",
    "    # prepare a mini-batch and take a gradient step to minimize -elbo\n",
    "    def process_minibatch(epoch, which_mini_batch, shuffled_indices):\n",
    "        if args.annealing_epochs > 0 and epoch < args.annealing_epochs:\n",
    "            # compute the KL annealing factor approriate for the current mini-batch in the current epoch\n",
    "            min_af = args.minimum_annealing_factor\n",
    "            annealing_factor = min_af + (1.0 - min_af) * \\\n",
    "                (float(which_mini_batch + epoch * N_mini_batches + 1) /\n",
    "                 float(args.annealing_epochs * N_mini_batches))\n",
    "        else:\n",
    "            # by default the KL annealing factor is unity\n",
    "            annealing_factor = 1.0\n",
    "\n",
    "        # compute which sequences in the training set we should grab\n",
    "        mini_batch_start = (which_mini_batch * args.mini_batch_size)\n",
    "        mini_batch_end = np.min([(which_mini_batch + 1) * args.mini_batch_size, N_train_data])\n",
    "        mini_batch_indices = shuffled_indices[mini_batch_start:mini_batch_end]\n",
    "        # grab a fully prepped mini-batch using the helper function in the data loader\n",
    "        mini_batch, mini_batch_reversed, mini_batch_mask, mini_batch_seq_lengths \\\n",
    "            = poly.get_mini_batch(mini_batch_indices, training_data_sequences,\n",
    "                                  training_seq_lengths, cuda=args.cuda)\n",
    "        # do an actual gradient step\n",
    "        loss = svi.step(mini_batch, mini_batch_reversed, mini_batch_mask,\n",
    "                        mini_batch_seq_lengths, annealing_factor)\n",
    "        # keep track of the training loss\n",
    "        return loss\n",
    "\n",
    "\n",
    "    # if checkpoint files provided, load model and optimizer states from disk before we start training\n",
    "    if args.load_opt != '' and args.load_model != '':\n",
    "        load_checkpoint()\n",
    "\n",
    "    #################\n",
    "    # TRAINING LOOP #\n",
    "    #################\n",
    "    times = [time.time()]\n",
    "    for epoch in range(args.num_epochs):\n",
    "        # if specified, save model and optimizer states to disk every checkpoint_freq epochs\n",
    "        if args.checkpoint_freq > 0 and epoch > 0 and epoch % args.checkpoint_freq == 0:\n",
    "            save_checkpoint()\n",
    "\n",
    "        # accumulator for our estimate of the negative log likelihood (or rather -elbo) for this epoch\n",
    "        epoch_nll = 0.0\n",
    "        # prepare mini-batch subsampling indices for this epoch\n",
    "        shuffled_indices = torch.randperm(N_train_data)\n",
    "\n",
    "        # process each mini-batch; this is where we take gradient steps\n",
    "        for which_mini_batch in range(N_mini_batches):\n",
    "            epoch_nll += process_minibatch(epoch, which_mini_batch, shuffled_indices)\n",
    "\n",
    "        # report training diagnostics\n",
    "        times.append(time.time())\n",
    "        epoch_time = times[-1] - times[-2]\n",
    "        logging.info(\"[training epoch %04d]  %.4f \\t\\t\\t\\t(dt = %.3f sec)\" %\n",
    "                     (epoch, epoch_nll / N_train_time_slices, epoch_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c6c4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"parse args\")\n",
    "    parser.add_argument('-n', '--num-epochs', type=int, default=5000)\n",
    "    parser.add_argument('-lr', '--learning-rate', type=float, default=0.0003)\n",
    "    parser.add_argument('-b1', '--beta1', type=float, default=0.96)\n",
    "    parser.add_argument('-b2', '--beta2', type=float, default=0.999)\n",
    "    parser.add_argument('-cn', '--clip-norm', type=float, default=10.0)\n",
    "    parser.add_argument('-lrd', '--lr-decay', type=float, default=0.99996)\n",
    "    parser.add_argument('-wd', '--weight-decay', type=float, default=2.0)\n",
    "    parser.add_argument('-mbs', '--mini-batch-size', type=int, default=20)\n",
    "    parser.add_argument('-ae', '--annealing-epochs', type=int, default=1000)\n",
    "    parser.add_argument('-maf', '--minimum-annealing-factor', type=float, default=0.2)\n",
    "    parser.add_argument('-rdr', '--rnn-dropout-rate', type=float, default=0.1)\n",
    "    parser.add_argument('-iafs', '--num-iafs', type=int, default=0)\n",
    "    parser.add_argument('-id', '--iaf-dim', type=int, default=100)\n",
    "    parser.add_argument('-cf', '--checkpoint-freq', type=int, default=0)\n",
    "    parser.add_argument('-lopt', '--load-opt', type=str, default='')\n",
    "    parser.add_argument('-lmod', '--load-model', type=str, default='')\n",
    "    parser.add_argument('-sopt', '--save-opt', type=str, default='')\n",
    "    parser.add_argument('-smod', '--save-model', type=str, default='')\n",
    "    parser.add_argument('--cuda', action='store_true')\n",
    "    parser.add_argument('--jit', action='store_true')\n",
    "    parser.add_argument('--tmc', action='store_true')\n",
    "    parser.add_argument('--tmcelbo', action='store_true')\n",
    "    parser.add_argument('--tmc-num-samples', default=10, type=int)\n",
    "    parser.add_argument('-l', '--log', type=str, default='dmm.log')\n",
    "    args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68270ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.from_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064f0fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.dataset_path = daset_path\n",
    "args.emission_matrix_path = emission_matrix_path\n",
    "args.model_path = model_path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef27b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
